sept 16, 2024

# motivating adam optimizer

our first type of optimization, the standard one size fits all solution
would be to subtract the gradient from the weights multiplied by some
learning rate constant throughout training. this changes the weights all the
same amount relative to their gradients. one obvious downside of this
approach, is it does not have any memory of prior gradient descent values
prior, so in effect, this can make the descent path more sparse. basically
the noise in the movement would be larger because we are not identifying
prior redundant paths based on a moving average, so if we are jostling from
high negative to high positive gradients and going across the whole error
surface for every iteration, we wouldn't be able to use this information in
standard gradient descent with a constant learning rate.

another thing which is bad about a constant learning rate, is that during
initial training, there are large gradients, and it would be helpful and
more computationally efficient to exponentially descrease the learning rate
as the training goes on since this could help us reduce the required number
of iterations.

# stochastic gradient descent

$$W_{t+1}=W_t-\eta\frac{\partial L}{\partial W_t}$$

This is the basic gradient descent formula. The next set of weights is equal
to what they were prior, minus the gradients times the learning rate.

# stochastic gradient descent with momentum

now, we can create memory of past gradient descent gradients in order to
smooth our descent through the error surface. we can do this with something
called momentum. momentum can be thought of how it is in the natural sense.
if you are barreling in one direction and suddenly make a weird pivot in the
opposite direction, you will slightly slow down your velocity in the
previous direction, but will still go in that direction for some number of
iterations, until there would be a sufficient number of sequential
iterations going towards that opposite direction. so momentum is the
direction and magnitude history of prior gradients. there are two ways to
implement momentum.
